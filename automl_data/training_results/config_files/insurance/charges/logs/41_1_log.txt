
0. Dataset reading
		Used delimiter: ,
		Initial shape of the dataset: (1338, 7)

1. Removing duplicated rows and useless columns
	1.a. Remove useless columns
		There are not useless columns.
	1.b. Remove duplicated rows
		Number of duplicated rows: 1.

2. Split dataset in training and testing
		Test percentage = 0.0
		Random state = 42
		Output column = charges
	1.c. Remove columns with missing data (after splitting)
		Every column has < 30% missing data.

3. Data types conversion for each column
		Code meanings:
			3a: bool -> int
			3b: int -> datetime
			3c: object -> datetime
			3d: object -> int/float
			3e: text normalization
			3f: without text normalization
		6.a. Creating additional features for text columns
		{
		    "sex": "3e",
		    "smoker": "3e",
		    "region": "3e"
		}

4. Dataset splitting based on categorical columns
		Used threshold = 0.002
		Categorical columns:            Datatype:            Ratio unique values:
		sex                             object               0.0014958863126402393
		smoker                          object               0.0014958863126402393
		region                          object               0.0029917726252804786
		 
		Continuous columns:             Datatype:            Ratio unique values:
		age                             int64                0.035153328347045626
		bmi                             float64              0.4098728496634256
		children                        int64                0.004487658937920718

5. Outliers detection
	5.a. Outliers detection in continuous columns
		three_std is the used method for outliers detection
		Column name:                    outliers    missing values:
		age                             0           0
		bmi                             4           0
		children                        18          0
		Lower and upper boundaries for outliers:
		{
		    "age": [
		        -9223372036854775807,
		        9223372036854775807
		    ],
		    "bmi": [
		        -9223372036854775807,
		        49.06
		    ],
		    "children": [
		        -9223372036854775807,
		        5
		    ]
		}
	5.b. Outliers detection in categorical columns
		Threshold for detecting categorical that will be verified for outliers: 0.1
		winsorization for 1st percentile is the method used for outliers detection
		Column name:                    outliers    missing values:
		sex                             0           0
		smoker                          0           0
		region                          0           0
		Lower boundary for outliers:
		{
		    "sex": 0,
		    "smoker": 0,
		    "region": 324.0
		}

6. Imputation of missing values
	6.a. Imputation for continuous data
		mean is used for imputation
		Imputed values:
		{
		    "age": 39.222139117427076,
		    "bmi": 30.60156414103526,
		    "children": 1.042456406368461
		}
	6.b. Imputation for categorical data
		continuous data was used: True
		mode is used for imputation
		Modes:
		{
		    "sex": "male",
		    "smoker": "no",
		    "region": "southeast"
		}

7. Feature engineering
	7.a. Generate cyclical date features

8. Encoding categorical features
		Column type                     column name                     encoding type
		shorttext                       sex                             0
		shorttext                       smoker                          0
		shorttext                       region                          0
	
Remove columns without variation

9. Scaling features
	9.a. Scaling continuous features
		Scaler type is StandardScaler
	9.b. Scaling categorical features
		Number of scalable columns: 0
		Scale entire categorical dataframe: False
		Scaler type is None
	7.g. Feature construction
		(age children /rev) => new_feat_0
		(age bmi_wo_skew /rev) => new_feat_1
		(age bmi_wo_skew *) => new_feat_2

9. Scaling features
	9.a. Scaling continuous features
		Scaler type is StandardScaler
		
Final shape of the training dataset: (1337, 13)		
Time required for preparing the training dataset: 0.686049 s
