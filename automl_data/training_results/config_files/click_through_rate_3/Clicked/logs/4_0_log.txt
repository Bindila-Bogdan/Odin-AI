
0. Dataset reading
		Used delimiter: ,
		Initial shape of the dataset: (796, 10)

1. Removing duplicated rows and useless columns
	1.a. Remove useless columns
		There are not useless columns.
	1.b. Remove duplicated rows
		The dataset does not contain duplicated rows.

2. Split dataset in training and testing
		Test percentage = 0.0
		Random state = 42
		Output column = Clicked
	1.c. Remove columns with missing data (after splitting)
		Every column has < 30% missing data.

3. Data types conversion for each column
		Code meanings:
			3a: bool -> int
			3b: int -> datetime
			3c: object -> datetime
			3d: object -> int/float
			3e: text normalization
			3f: without text normalization
		6.a. Creating additional features for text columns
		{
		    "Ad topic line": "3e",
		    "City": "3e",
		    "Country": "3e",
		    "Timestamp": "3c"
		}

4. Dataset splitting based on categorical columns
		Used threshold = 0.002
		Categorical columns:            Datatype:            Ratio unique values:
		Ad topic line                   object               1.0
		City                            object               0.9711055276381909
		Male                            int64                0.002512562814070352
		Country                         object               0.2889447236180904
		Timestamp                       datetime64[ns]       0.9962311557788944
		 
		Continuous columns:             Datatype:            Ratio unique values:
		Daily site usage                float64              0.9195979899497487
		Age                             int64                0.05402010050251256
		Area income                     float64              1.0
		Daily internet usage            float64              0.9685929648241206
		Ad topic line_chars_no          int64                0.04396984924623116
		Ad topic line_avg_word_len      float64              0.08417085427135679
		City_chars_no                   int64                0.018844221105527637
		City_avg_word_len               float64              0.026381909547738693
		Country_chars_no                int64                0.035175879396984924
		Country_avg_word_len            float64              0.03391959798994975

5. Outliers detection
	5.a. Outliers detection in continuous columns
		iqr is the used method for outliers detection
		Column name:                    outliers    missing values:
		Daily site usage                0           0
		Age                             4           0
		Area income                     14          0
		Daily internet usage            0           0
		Lower and upper boundaries for outliers:
		{
		    "Daily site usage": [
		        -9223372036854775807,
		        9223372036854775807
		    ],
		    "Age": [
		        -9223372036854775807,
		        60
		    ],
		    "Area income": [
		        22473.08,
		        9223372036854775807
		    ],
		    "Daily internet usage": [
		        -9223372036854775807,
		        9223372036854775807
		    ]
		}
	5.b. Without outliers detection in categorical columns

6. Imputation of missing values
	6.a. Imputation for continuous data
		mean is used for imputation
		Imputed values:
		{
		    "Daily site usage": 65.13646984924624,
		    "Age": 35.65909090909091,
		    "Area income": 56057.666061381075,
		    "Daily internet usage": 179.75201005025122,
		    "Ad topic line_chars_no": 32.314070351758794,
		    "Ad topic line_avg_word_len": 8.66529373055755,
		    "City_chars_no": 11.881909547738694,
		    "City_avg_word_len": 8.701005025125628,
		    "Country_chars_no": 9.889447236180905,
		    "Country_avg_word_len": 7.027554438860973
		}
	6.b. Imputation for categorical data
		continuous data was used: True
		mode is used for imputation
		Modes:
		{
		    "Ad topic line": "profound line standardization",
		    "City": "lake susan",
		    "Male": 0,
		    "Country": "liberia",
		    "Timestamp": "2016-05-30 08:02:00"
		}

7. Feature engineering
	7.a. Generate cyclical date features
		Column name: Timestamp with max_month = 11 and max_day = 30
	7.b. Text feature engineering applied
		7.e. Reducing dimensionality of tf_idf result

8. Encoding categorical features
		Column type                     column name                     encoding type
		
		Ad topic line trying with min_df=0.01 max_df=0.99
		for Ad topic line tf-idf with min_df=0.01 max_df=0.99
		tf_idf for Ad topic line has initially 192 columns
		Try 96 components. Explained variance: 0.7394174797526321
		Try 144 components. Explained variance: 0.9329080950897334
		Explained variance for Ad topic line: 0.9329080950897334
		
		City trying with min_df=0.01 max_df=0.99
		for City tf-idf with min_df=0.01 max_df=0.99
		tf_idf for City has initially 7 columns
		Try 4 components. Explained variance: 0.62779838564516
		Try 5 components. Explained variance: 0.7667832550391276
		Try 6 components. Explained variance: 0.9008834128040171
		Explained variance for City: 0.9008834128040171
		
		Country trying with min_df=0.01 max_df=0.99
		for Country tf-idf with min_df=0.01 max_df=0.99
		tf_idf for Country has initially 13 columns
		Try 7 components. Explained variance: 0.7357424726179528
		Try 10 components. Explained variance: 0.9011299150432367
		Explained variance for Country: 0.9011299150432367
		longtext                        Ad topic line                   1
		longtext                        City                            1
		num                             Male                            1
		longtext                        Country                         1
		svd                                                             Ad topic line
		svd                                                             City
		svd                                                             Country
		Moving column Ad topic line_svd_0 from categorical to continuous
	
Remove columns without variation
		Column Timestamp_year has zero variance.

9. Scaling features
	9.a. Scaling continuous features
		Scaler type is MinMaxScaler
	9.b. Scaling categorical features
		Number of scalable columns: 0
		Scale entire categorical dataframe: True
		Scaler type is MinMaxScaler
	7.f. Feature selection
		Discarded features are: ['City_avg_word_len', 'Country_chars_no', 'Country_avg_word_len', 'City_chars_no_wo_skew', 'Timestamp_month_cos', 'Timestamp_day_sin', 'Timestamp_day_cos', 'Ad topic line_svd_0', 'Ad topic line_svd_1', 'Ad topic line_svd_3', 'Ad topic line_svd_4', 'Ad topic line_svd_5', 'Ad topic line_svd_6', 'Ad topic line_svd_7', 'Ad topic line_svd_9', 'Ad topic line_svd_10', 'Ad topic line_svd_13', 'Ad topic line_svd_14', 'Ad topic line_svd_15', 'Ad topic line_svd_16', 'Ad topic line_svd_17', 'Ad topic line_svd_19', 'Ad topic line_svd_20', 'Ad topic line_svd_25', 'Ad topic line_svd_28', 'Ad topic line_svd_29', 'Ad topic line_svd_30', 'Ad topic line_svd_31', 'Ad topic line_svd_33', 'Ad topic line_svd_37', 'Ad topic line_svd_38', 'Ad topic line_svd_40', 'Ad topic line_svd_42', 'Ad topic line_svd_43', 'Ad topic line_svd_45', 'Ad topic line_svd_46', 'Ad topic line_svd_48', 'Ad topic line_svd_49', 'Ad topic line_svd_50', 'Ad topic line_svd_51', 'Ad topic line_svd_52', 'Ad topic line_svd_53', 'Ad topic line_svd_54', 'Ad topic line_svd_56', 'Ad topic line_svd_57', 'Ad topic line_svd_61', 'Ad topic line_svd_63', 'Ad topic line_svd_68', 'Ad topic line_svd_72', 'Ad topic line_svd_73', 'Ad topic line_svd_78', 'Ad topic line_svd_80', 'Ad topic line_svd_81', 'Ad topic line_svd_86', 'Ad topic line_svd_87', 'Ad topic line_svd_89', 'Ad topic line_svd_90', 'Ad topic line_svd_91', 'Ad topic line_svd_92', 'Ad topic line_svd_93', 'Ad topic line_svd_94', 'Ad topic line_svd_95', 'Ad topic line_svd_96', 'Ad topic line_svd_97', 'Ad topic line_svd_98', 'Ad topic line_svd_101', 'Ad topic line_svd_102', 'Ad topic line_svd_103', 'Ad topic line_svd_107', 'Ad topic line_svd_108', 'Ad topic line_svd_109', 'Ad topic line_svd_112', 'Ad topic line_svd_113', 'Ad topic line_svd_114', 'Ad topic line_svd_115', 'Ad topic line_svd_116', 'Ad topic line_svd_118', 'Ad topic line_svd_120', 'Ad topic line_svd_121', 'Ad topic line_svd_123', 'Ad topic line_svd_125', 'Ad topic line_svd_126', 'Ad topic line_svd_128', 'Ad topic line_svd_130', 'Ad topic line_svd_131', 'Ad topic line_svd_135', 'Ad topic line_svd_137', 'Ad topic line_svd_138', 'Ad topic line_svd_140', 'Ad topic line_svd_141', 'Ad topic line_svd_142', 'City_svd_1', 'City_svd_2', 'City_svd_5', 'Country_svd_0', 'Country_svd_1', 'Country_svd_2', 'Country_svd_5', 'Country_svd_6', 'Country_svd_7', 'Country_svd_9', 'Male_enc_0', 'Male_enc_1']
	7.g. Feature construction
		(Daily internet usage Ad topic line_svd_104 /) => new_feat_0
		(Daily internet usage Daily site usage /) => new_feat_1
		(Daily internet usage Daily site usage /rev) => new_feat_2

9. Scaling features
	9.a. Scaling continuous features
		Scaler type is MinMaxScaler
		
Final shape of the training dataset: (796, 76)		
Time required for preparing the training dataset: 4.937196 s
