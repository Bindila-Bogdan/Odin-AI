
0. Dataset reading
		Used delimiter: ,
		Initial shape of the dataset: (1338, 7)

1. Removing duplicated rows and useless columns
	1.a. Remove useless columns
		There are not useless columns.
	1.b. Remove duplicated rows
		Number of duplicated rows: 1.

2. Split dataset in training and testing
		Test percentage = 0.0
		Random state = 42
		Output column = charges
	1.c. Remove columns with missing data (after splitting)
		Every column has < 30% missing data.

3. Data types conversion for each column
		Code meanings:
			3a: bool -> int
			3b: int -> datetime
			3c: object -> datetime
			3d: object -> int/float
			3e: text normalization
			3f: without text normalization
		6.a. Creating additional features for text columns
		{
		    "sex": "3e",
		    "smoker": "3e",
		    "region": "3e"
		}

4. Dataset splitting based on categorical columns
		Used threshold = 0.002
		Categorical columns:            Datatype:            Ratio unique values:
		sex                             object               0.0014958863126402393
		smoker                          object               0.0014958863126402393
		region                          object               0.0029917726252804786
		 
		Continuous columns:             Datatype:            Ratio unique values:
		age                             int64                0.035153328347045626
		bmi                             float64              0.4098728496634256
		children                        int64                0.004487658937920718

5. Outliers detection
	5.a. Without outliers detection in continuous columns
	5.b. Outliers detection in categorical columns
		Threshold for detecting categorical that will be verified for outliers: 0.1
		winsorization for 1st percentile is the method used for outliers detection
		Column name:                    outliers    missing values:
		sex                             0           0
		smoker                          0           0
		region                          0           0
		Lower boundary for outliers:
		{
		    "sex": 0,
		    "smoker": 0,
		    "region": 324.0
		}

6. Imputation of missing values
	6.a. Imputation for continuous data
		mean is used for imputation
		Imputed values:
		{
		    "age": 39.222139117427076,
		    "bmi": 30.66345175766642,
		    "children": 1.0957367240089753
		}
	6.b. Imputation for categorical data
		continuous data was used: False
		kNN with 1 neighbors is used for imputation

7. Feature engineering
	7.a. Generate cyclical date features
	7.b. Text feature engineering applied
		7.e. Reducing dimensionality of tf_idf result

8. Encoding categorical features
		Column type                     column name                     encoding type
		shorttext                       sex                             0
		shorttext                       smoker                          0
		shorttext                       region                          0
	
Remove columns without variation

9. Scaling features
	9.a. Scaling continuous features
		Scaler type is StandardScaler
	9.b. Scaling categorical features
		Number of scalable columns: 0
		Scale entire categorical dataframe: False
		Scaler type is None
	7.g. Feature construction
		(age children_wo_skew /rev) => new_feat_0
		(age bmi_wo_skew /) => new_feat_1
		(age bmi_wo_skew *) => new_feat_2

9. Scaling features
	9.a. Scaling continuous features
		Scaler type is StandardScaler
		
Final shape of the training dataset: (1337, 13)		
Time required for preparing the training dataset: 0.504675 s
